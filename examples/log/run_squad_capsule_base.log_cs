nohup: ignoring input
05/20/2019 18:58:44 - INFO - root -   Run with args:
{
  "bert_model": "../../pretrained_models/uncased_L-12_H-768_A-12",
  "do_lower_case": true,
  "do_predict": true,
  "do_train": true,
  "doc_stride": 128,
  "eveluate_every_step": 3000,
  "fp16": false,
  "gradient_accumulation_steps": 24,
  "learning_rate": 5e-05,
  "local_rank": -1,
  "log_every_step": 100,
  "loss_scale": 0,
  "max_answer_length": 30,
  "max_query_length": 64,
  "max_seq_length": 300,
  "n_best_size": 20,
  "no_cuda": false,
  "null_score_diff_threshold": 1.0,
  "num_train_epochs": 3.0,
  "output_dir": "../../output/log_eval",
  "predict_batch_size": 8,
  "predict_file": "../../data/dev-v2.0.json",
  "save_model_every_step": 6000,
  "seed": 42,
  "server_ip": "",
  "server_port": "",
  "train_batch_size": 24,
  "train_file": "../../data/train-v2.0.json",
  "verbose_logging": false,
  "version_2_with_negative": true,
  "warmup_proportion": 0.1
}
05/20/2019 18:58:44 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
05/20/2019 18:58:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../../pretrained_models/uncased_L-12_H-768_A-12/vocab.txt
05/20/2019 18:58:51 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ../../pretrained_models/uncased_L-12_H-768_A-12
05/20/2019 18:58:51 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

/home/chen-ubuntu/anaconda3/envs/zsw_capsule/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
05/20/2019 18:58:53 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['capsule.primary_capsules.capsules.0.weight', 'capsule.primary_capsules.capsules.0.bias', 'capsule.primary_capsules.capsules.1.weight', 'capsule.primary_capsules.capsules.1.bias', 'capsule.primary_capsules.capsules.2.weight', 'capsule.primary_capsules.capsules.2.bias', 'capsule.primary_capsules.capsules.3.weight', 'capsule.primary_capsules.capsules.3.bias', 'capsule.primary_capsules.capsules.4.weight', 'capsule.primary_capsules.capsules.4.bias', 'capsule.primary_capsules.capsules.5.weight', 'capsule.primary_capsules.capsules.5.bias', 'capsule.primary_capsules.capsules.6.weight', 'capsule.primary_capsules.capsules.6.bias', 'capsule.primary_capsules.capsules.7.weight', 'capsule.primary_capsules.capsules.7.bias', 'capsule.digit_capsules.route_weights']
05/20/2019 18:58:53 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
05/20/2019 18:58:55 - INFO - __main__ -   trained parameter list:
bert.embeddings.word_embeddings.weight                                          [30522, 768]                                                                    
bert.embeddings.position_embeddings.weight                                      [512, 768]                                                                      
bert.embeddings.token_type_embeddings.weight                                    [2, 768]                                                                        
bert.embeddings.LayerNorm.weight                                                [768]                                                                           
bert.embeddings.LayerNorm.bias                                                  [768]                                                                           
bert.encoder.layer.0.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.0.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.0.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.0.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.0.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.0.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.0.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.0.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.0.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.0.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.0.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.0.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.0.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.0.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.0.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.0.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.1.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.1.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.1.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.1.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.1.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.1.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.1.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.1.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.1.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.1.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.1.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.1.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.1.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.1.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.1.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.1.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.2.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.2.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.2.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.2.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.2.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.2.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.2.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.2.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.2.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.2.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.2.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.2.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.2.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.2.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.2.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.2.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.3.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.3.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.3.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.3.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.3.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.3.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.3.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.3.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.3.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.3.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.3.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.3.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.3.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.3.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.3.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.3.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.4.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.4.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.4.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.4.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.4.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.4.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.4.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.4.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.4.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.4.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.4.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.4.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.4.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.4.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.4.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.4.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.5.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.5.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.5.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.5.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.5.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.5.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.5.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.5.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.5.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.5.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.5.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.5.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.5.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.5.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.5.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.5.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.6.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.6.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.6.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.6.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.6.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.6.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.6.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.6.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.6.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.6.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.6.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.6.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.6.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.6.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.6.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.6.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.7.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.7.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.7.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.7.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.7.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.7.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.7.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.7.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.7.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.7.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.7.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.7.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.7.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.7.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.7.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.7.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.8.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.8.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.8.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.8.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.8.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.8.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.8.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.8.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.8.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.8.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.8.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.8.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.8.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.8.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.8.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.8.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.9.attention.self.query.weight                                [768, 768]                                                                      
bert.encoder.layer.9.attention.self.query.bias                                  [768]                                                                           
bert.encoder.layer.9.attention.self.key.weight                                  [768, 768]                                                                      
bert.encoder.layer.9.attention.self.key.bias                                    [768]                                                                           
bert.encoder.layer.9.attention.self.value.weight                                [768, 768]                                                                      
bert.encoder.layer.9.attention.self.value.bias                                  [768]                                                                           
bert.encoder.layer.9.attention.output.dense.weight                              [768, 768]                                                                      
bert.encoder.layer.9.attention.output.dense.bias                                [768]                                                                           
bert.encoder.layer.9.attention.output.LayerNorm.weight                          [768]                                                                           
bert.encoder.layer.9.attention.output.LayerNorm.bias                            [768]                                                                           
bert.encoder.layer.9.intermediate.dense.weight                                  [3072, 768]                                                                     
bert.encoder.layer.9.intermediate.dense.bias                                    [3072]                                                                          
bert.encoder.layer.9.output.dense.weight                                        [768, 3072]                                                                     
bert.encoder.layer.9.output.dense.bias                                          [768]                                                                           
bert.encoder.layer.9.output.LayerNorm.weight                                    [768]                                                                           
bert.encoder.layer.9.output.LayerNorm.bias                                      [768]                                                                           
bert.encoder.layer.10.attention.self.query.weight                               [768, 768]                                                                      
bert.encoder.layer.10.attention.self.query.bias                                 [768]                                                                           
bert.encoder.layer.10.attention.self.key.weight                                 [768, 768]                                                                      
bert.encoder.layer.10.attention.self.key.bias                                   [768]                                                                           
bert.encoder.layer.10.attention.self.value.weight                               [768, 768]                                                                      
bert.encoder.layer.10.attention.self.value.bias                                 [768]                                                                           
bert.encoder.layer.10.attention.output.dense.weight                             [768, 768]                                                                      
bert.encoder.layer.10.attention.output.dense.bias                               [768]                                                                           
bert.encoder.layer.10.attention.output.LayerNorm.weight                         [768]                                                                           
bert.encoder.layer.10.attention.output.LayerNorm.bias                           [768]                                                                           
bert.encoder.layer.10.intermediate.dense.weight                                 [3072, 768]                                                                     
bert.encoder.layer.10.intermediate.dense.bias                                   [3072]                                                                          
bert.encoder.layer.10.output.dense.weight                                       [768, 3072]                                                                     
bert.encoder.layer.10.output.dense.bias                                         [768]                                                                           
bert.encoder.layer.10.output.LayerNorm.weight                                   [768]                                                                           
bert.encoder.layer.10.output.LayerNorm.bias                                     [768]                                                                           
bert.encoder.layer.11.attention.self.query.weight                               [768, 768]                                                                      
bert.encoder.layer.11.attention.self.query.bias                                 [768]                                                                           
bert.encoder.layer.11.attention.self.key.weight                                 [768, 768]                                                                      
bert.encoder.layer.11.attention.self.key.bias                                   [768]                                                                           
bert.encoder.layer.11.attention.self.value.weight                               [768, 768]                                                                      
bert.encoder.layer.11.attention.self.value.bias                                 [768]                                                                           
bert.encoder.layer.11.attention.output.dense.weight                             [768, 768]                                                                      
bert.encoder.layer.11.attention.output.dense.bias                               [768]                                                                           
bert.encoder.layer.11.attention.output.LayerNorm.weight                         [768]                                                                           
bert.encoder.layer.11.attention.output.LayerNorm.bias                           [768]                                                                           
bert.encoder.layer.11.intermediate.dense.weight                                 [3072, 768]                                                                     
bert.encoder.layer.11.intermediate.dense.bias                                   [3072]                                                                          
bert.encoder.layer.11.output.dense.weight                                       [768, 3072]                                                                     
bert.encoder.layer.11.output.dense.bias                                         [768]                                                                           
bert.encoder.layer.11.output.LayerNorm.weight                                   [768]                                                                           
bert.encoder.layer.11.output.LayerNorm.bias                                     [768]                                                                           
bert.pooler.dense.weight                                                        [768, 768]                                                                      
bert.pooler.dense.bias                                                          [768]                                                                           
capsule.primary_capsules.capsules.0.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.0.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.1.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.1.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.2.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.2.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.3.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.3.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.4.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.4.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.5.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.5.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.6.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.6.bias                                        [32]                                                                            
capsule.primary_capsules.capsules.7.weight                                      [32, 1, 3, 768]                                                                 
capsule.primary_capsules.capsules.7.bias                                        [32]                                                                            
capsule.digit_capsules.route_weights                                            [2, 4768, 8, 300]                                                               

05/20/2019 18:59:07 - INFO - __main__ -   ***** Running training *****
05/20/2019 18:59:07 - INFO - __main__ -     Num orig examples = 130319
05/20/2019 18:59:07 - INFO - __main__ -     Num split examples = 136760
05/20/2019 18:59:07 - INFO - __main__ -     Batch size = 1
05/20/2019 18:59:07 - INFO - __main__ -     Num steps = 16287
05/20/2019 18:59:10 - INFO - __main__ -   start training..........................
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]05/20/2019 18:59:10 - INFO - __main__ -   epoch 1
start_positions: tensor([46], device='cuda:0')
end_positions: tensor([48], device='cuda:0')
logits_pred: tensor([[-1.8441e-04,  4.1823e-05, -3.4911e-04,  1.2962e-04,  4.8468e-05,
         -7.6197e-04, -1.2476e-04, -4.6685e-05, -1.2551e-04, -3.8367e-04,
          5.3137e-06,  2.3873e-04, -4.3781e-04, -4.8530e-04,  7.8744e-04,
          2.0512e-04, -1.3861e-04, -8.5709e-05, -8.5949e-05,  4.8591e-04,
         -2.3746e-04, -1.2914e-04, -4.0376e-04, -2.3892e-04, -2.8456e-04,
         -1.8386e-04,  7.7836e-04,  3.3031e-04, -1.2266e-05,  3.2252e-04,
         -2.0364e-04, -9.9771e-05, -1.6259e-04,  4.1998e-04, -1.4431e-04,
          4.8651e-04,  9.4616e-05, -1.5613e-04, -2.0433e-04,  3.9778e-05,
         -5.5629e-04,  7.6145e-05, -4.1032e-04,  3.3800e-04,  4.6877e-04,
         -5.8533e-04,  1.1592e-04, -3.7964e-04, -1.0498e-04, -1.3818e-04,
          6.9011e-04, -4.9210e-04, -5.3762e-04, -4.8077e-04,  3.9020e-05,
         -1.5629e-04, -2.0869e-04, -6.8703e-05, -8.3773e-06, -2.1403e-04,
          3.5916e-04, -6.8073e-04, -1.7417e-04,  4.4398e-04,  4.9109e-04,
          8.9185e-05,  3.4148e-04, -2.6382e-04, -3.4928e-04, -2.3088e-04,
         -4.4328e-05,  2.5825e-04,  1.5507e-04, -5.2474e-04, -4.3740e-04,
          1.4131e-04, -4.0098e-04, -1.8479e-05, -4.9823e-04,  1.6752e-04,
         -2.6843e-04, -1.1452e-04, -2.6480e-04,  2.0324e-04,  5.2741e-05,
         -4.0128e-04, -4.2053e-04,  1.7725e-04, -2.6532e-04,  1.8698e-05,
         -5.4270e-04,  2.8853e-04, -9.9031e-05, -8.2921e-04, -3.8176e-04,
         -1.7764e-04, -3.3108e-04,  2.8127e-04, -4.9262e-04,  2.9424e-04,
         -4.3291e-05,  2.4331e-04,  3.3171e-04, -2.2485e-04,  5.1957e-04,
         -2.9094e-04,  1.3736e-04, -6.2486e-05,  1.1524e-05, -9.9318e-05,
         -5.5719e-06,  1.7231e-04,  1.1675e-04, -3.8821e-04,  1.9319e-04,
          5.4572e-04, -1.3582e-04,  3.4566e-06,  2.5934e-04,  6.8529e-05,
          3.3091e-05,  5.4557e-05, -4.4403e-04, -3.3676e-04,  4.8971e-05,
          2.9031e-04, -1.5976e-04, -2.7005e-04, -2.0350e-04,  5.5502e-06,
         -5.1676e-04,  3.0364e-04, -2.5274e-04, -1.8446e-04,  4.0502e-04,
          1.3286e-04, -8.6556e-05, -8.9286e-04, -4.7294e-04, -3.8198e-05,
          2.5535e-05,  3.9846e-04,  2.3247e-04, -1.8390e-04, -2.5938e-04,
         -5.9169e-04,  7.8443e-05, -3.8453e-04, -2.7538e-04, -2.8045e-04,
          1.5979e-04, -3.3090e-04,  5.6593e-06,  2.2145e-04,  1.9337e-04,
          4.7786e-04,  4.3908e-04,  6.6174e-04,  1.4822e-04,  3.9510e-05,
          2.8801e-04, -3.2635e-05, -3.4542e-04, -2.0367e-04, -3.5482e-05,
          2.8227e-04,  5.3029e-05, -2.3979e-05,  5.1331e-04, -5.5917e-04,
          8.3603e-05,  4.0161e-04, -3.1450e-04,  5.3400e-04, -1.9619e-04,
         -5.7139e-04, -2.9526e-04, -4.5662e-04, -2.7925e-04, -3.8433e-04,
         -2.1435e-04,  1.9206e-04, -1.1514e-04, -2.0698e-04,  1.0314e-04,
         -3.8531e-05,  1.2682e-04,  1.0142e-04, -5.7334e-04, -2.1931e-04,
         -5.1746e-05,  8.9213e-05, -2.7118e-04,  2.8306e-04,  1.4185e-04,
          3.2772e-04,  1.0804e-04, -3.9948e-04, -1.0926e-04, -6.0735e-04,
          2.8190e-05, -1.2732e-05, -5.0698e-04, -8.4998e-05,  2.0903e-05,
         -3.7962e-04,  2.2910e-04,  2.7642e-04, -1.4814e-04, -3.2096e-05,
         -2.8967e-04,  5.0195e-04,  1.8529e-04, -1.5157e-04,  9.7273e-05,
         -2.8073e-04, -1.7593e-04,  4.9958e-05, -1.1928e-04,  4.7243e-05,
         -4.6035e-05,  9.1799e-05,  4.9553e-04,  3.3973e-04, -4.6780e-05,
          6.6035e-05, -4.7218e-04,  1.5436e-04, -9.6430e-05, -4.9391e-04,
         -4.0129e-05,  4.4601e-04, -6.6506e-05,  1.1400e-04, -1.6790e-04,
          2.7086e-04, -6.0829e-06, -8.9263e-05,  4.8304e-04, -6.3222e-05,
          7.8963e-05,  1.2122e-05,  1.0116e-04, -2.0147e-04,  3.2257e-04,
          1.1162e-04,  5.2091e-06,  1.4228e-06, -3.1378e-04, -5.6725e-05,
          1.4299e-04,  2.0036e-04, -2.2373e-05, -1.9709e-04,  8.6850e-05,
          2.2006e-04, -2.1823e-04,  1.3802e-05,  2.1640e-04,  8.6131e-05,
         -1.4514e-05, -5.2476e-04, -4.3294e-05,  1.2916e-04,  2.2614e-04,
          8.8589e-04, -1.6257e-04, -2.2780e-04, -7.8345e-04, -3.6680e-04,
         -5.6768e-04,  1.8276e-04,  8.0252e-04,  2.9644e-04, -4.1606e-04,
          3.0829e-04, -2.6974e-04,  2.8858e-04,  4.8712e-04,  1.2430e-04,
          2.9027e-04,  3.5255e-04, -3.4271e-04,  1.8350e-04,  2.6072e-04,
          1.1582e-04, -3.6506e-04,  2.2306e-04,  2.1530e-04,  6.3990e-05,
         -5.4328e-05,  3.2304e-04,  4.0802e-04, -3.0126e-04,  2.9569e-04,
         -5.8778e-04, -9.6325e-05, -7.3902e-05, -3.8286e-04, -2.5916e-04]],
       device='cuda:0', grad_fn=<ViewBackward>)
probs_pred: tensor([[7.7691e-06, 7.4627e-05, 1.4965e-06, 1.7955e-04, 7.9754e-05, 2.4102e-08,
         1.4107e-05, 3.0797e-05, 1.4001e-05, 1.0593e-06, 5.1801e-05, 5.3465e-04,
         6.1644e-07, 3.8338e-07, 1.2914e-01, 3.8202e-04, 1.2282e-05, 2.0846e-05,
         2.0796e-05, 6.3318e-03, 4.5705e-06, 1.3502e-05, 8.6643e-07, 4.5043e-06,
         2.8537e-06, 7.8123e-06, 1.1793e-01, 1.3359e-03, 4.3450e-05, 1.2358e-03,
         6.4100e-06, 1.8112e-05, 9.6640e-06, 3.2751e-03, 1.1602e-05, 6.3704e-03,
         1.2652e-04, 1.0308e-05, 6.3662e-06, 7.3116e-05, 1.8851e-07, 1.0518e-04,
         8.1147e-07, 1.4426e-03, 5.3344e-03, 1.4099e-07, 1.5657e-04, 1.1028e-06,
         1.7193e-05, 1.2336e-05, 4.8793e-02, 3.5816e-07, 2.2719e-07, 4.0114e-07,
         7.2564e-05, 1.0292e-05, 6.0941e-06, 2.4711e-05, 4.5173e-05, 5.7774e-06,
         1.7827e-03, 5.4312e-08, 8.6072e-06, 4.1634e-03, 6.6688e-03, 1.1983e-04,
         1.4938e-03, 3.5114e-06, 1.4940e-06, 4.8814e-06, 3.1531e-05, 6.4988e-04,
         2.3158e-04, 2.5842e-07, 6.1892e-07, 2.0183e-04, 8.9086e-07, 4.0832e-05,
         3.3687e-07, 2.6230e-04, 3.3533e-06, 1.5628e-05, 3.4773e-06, 3.7490e-04,
         8.3236e-05, 8.8824e-07, 7.3267e-07, 2.8909e-04, 3.4594e-06, 5.9219e-05,
         2.1593e-07, 8.7965e-04, 1.8246e-05, 1.2304e-08, 1.0796e-06, 8.3135e-06,
         1.7923e-06, 8.1811e-04, 3.5631e-07, 9.3139e-04, 3.1860e-05, 5.5966e-04,
         1.3547e-03, 5.1849e-06, 8.8661e-03, 2.6773e-06, 1.9400e-04, 2.6296e-05,
         5.5119e-05, 1.8194e-05, 4.6458e-05, 2.7516e-04, 1.5787e-04, 1.0122e-06,
         3.3907e-04, 1.1516e-02, 1.2630e-05, 5.0847e-05, 6.5698e-04, 9.7471e-05,
         6.8386e-05, 8.4761e-05, 5.7923e-07, 1.6933e-06, 8.0156e-05, 8.9549e-04,
         9.9411e-06, 3.2994e-06, 6.4190e-06, 5.1923e-05, 2.7989e-07, 1.0231e-03,
         3.9232e-06, 7.7655e-06, 2.8198e-03, 1.8546e-04, 2.0671e-05, 6.5104e-09,
         4.3380e-07, 3.3525e-05, 6.3410e-05, 2.6410e-03, 5.0218e-04, 7.8089e-06,
         3.6711e-06, 1.3230e-07, 1.0763e-04, 1.0502e-06, 3.1281e-06, 2.9737e-06,
         2.4277e-04, 1.7955e-06, 5.1980e-05, 4.4979e-04, 3.3968e-04, 5.8421e-03,
         3.9642e-03, 3.6742e-02, 2.1626e-04, 7.2920e-05, 8.7510e-04, 3.5442e-05,
         1.5528e-06, 6.4081e-06, 3.4448e-05, 8.2633e-04, 8.3476e-05, 3.8647e-05,
         8.3281e-03, 1.8316e-07, 1.1333e-04, 2.7255e-03, 2.1153e-06, 1.0242e-02,
         6.9060e-06, 1.6209e-07, 2.5641e-06, 5.1072e-07, 3.0095e-06, 1.0522e-06,
         5.7589e-06, 3.3526e-04, 1.5531e-05, 6.1994e-06, 1.3778e-04, 3.3413e-05,
         1.7460e-04, 1.3543e-04, 1.5895e-07, 5.4805e-06, 2.9277e-05, 1.1987e-04,
         3.2625e-06, 8.3286e-04, 2.0291e-04, 1.3017e-03, 1.4470e-04, 9.0439e-07,
         1.6472e-05, 1.1313e-07, 6.5115e-05, 4.3248e-05, 3.0866e-07, 2.0995e-05,
         6.0539e-05, 1.1031e-06, 4.8557e-04, 7.7937e-04, 1.1166e-05, 3.5634e-05,
         2.7118e-06, 7.4333e-03, 3.1330e-04, 1.0790e-05, 1.2993e-04, 2.9653e-06,
         8.4572e-06, 8.0951e-05, 1.4902e-05, 7.8783e-05, 3.0998e-05, 1.2301e-04,
         6.9712e-03, 1.4678e-03, 3.0768e-05, 9.5070e-05, 4.3714e-07, 2.2995e-04,
         1.8727e-05, 3.5175e-07, 3.2884e-05, 4.2485e-03, 2.5259e-05, 1.5358e-04,
         9.1642e-06, 7.3723e-04, 4.6221e-05, 2.0118e-05, 6.1528e-03, 2.6103e-05,
         1.0819e-04, 5.5450e-05, 1.3508e-04, 6.5505e-06, 1.2364e-03, 1.4997e-04,
         5.1746e-05, 4.9824e-05, 2.1308e-06, 2.7855e-05, 2.0524e-04, 3.6426e-04,
         3.9273e-05, 6.8442e-06, 1.1707e-04, 4.4357e-04, 5.5400e-06, 5.6389e-05,
         4.2763e-04, 1.1623e-04, 4.2484e-05, 2.5837e-07, 3.1859e-05, 1.7874e-04,
         4.7136e-04, 3.4564e-01, 9.6659e-06, 5.0344e-06, 1.9443e-08, 1.2540e-06,
         1.6822e-07, 3.0546e-04, 1.5016e-01, 9.5213e-04, 7.6618e-07, 1.0719e-03,
         3.3096e-06, 8.8012e-04, 6.4093e-03, 1.7025e-04, 8.9515e-04, 1.6687e-03,
         1.5955e-06, 3.0775e-04, 6.6612e-04, 1.5640e-04, 1.2759e-06, 4.5710e-04,
         4.2296e-04, 9.3145e-05, 2.8531e-05, 1.2422e-03, 2.9059e-03, 2.4149e-06,
         9.4499e-04, 1.3758e-07, 1.8747e-05, 2.3459e-05, 1.0679e-06, 3.6789e-06]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
labels: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')
logits_pred: tensor([[ 7.9811e-05,  1.7781e-04, -3.7857e-04, -8.5019e-05,  3.3399e-04,
         -5.8819e-04, -5.1515e-04,  4.7736e-04, -1.4430e-04,  1.4817e-04,
         -2.1379e-04,  1.3423e-04,  1.6232e-04, -2.6600e-05, -3.0527e-04,
          8.0819e-05,  3.9142e-05, -3.4387e-04, -1.7161e-04,  2.0217e-04,
          1.7946e-04, -4.4830e-05, -1.6422e-04,  1.4905e-04, -2.5758e-04,
          6.1037e-04, -2.4924e-04,  1.3334e-04, -3.8615e-05, -4.2766e-04,
         -2.8841e-04, -1.4504e-04,  4.3170e-04, -1.1330e-04, -1.3637e-05,
         -2.6927e-04, -5.4659e-05,  2.2905e-04,  2.6571e-04, -1.4943e-04,
          1.0357e-04, -3.9795e-04,  4.5686e-04, -3.8057e-05, -2.4571e-05,
          3.5347e-05, -1.5290e-04, -8.4881e-05,  1.1030e-04,  3.3492e-04,
         -4.5344e-04, -5.6816e-04,  6.7193e-04,  5.1225e-04, -2.3692e-04,
          1.4755e-04,  2.6985e-04, -2.5444e-04,  3.8835e-06, -3.6785e-04,
          5.7545e-05, -3.7651e-04,  6.0831e-04,  1.4186e-04, -4.1995e-05,
          2.5434e-04,  3.1988e-05, -1.5855e-04, -1.4343e-04,  1.8810e-04,
         -1.5295e-04, -8.8981e-05,  4.7969e-04,  3.6856e-04,  4.8673e-04,
          2.9261e-04, -1.3043e-04, -1.4987e-04,  3.6268e-05, -1.8273e-04,
         -4.3121e-04, -3.4780e-04, -2.2505e-04, -2.7116e-04,  2.4266e-05,
         -7.5450e-05, -2.5310e-04,  1.1295e-04,  1.5446e-04, -2.9858e-05,
          3.1663e-04, -4.4531e-06,  1.4753e-04, -7.1310e-05, -1.9165e-04,
          2.5322e-04,  3.2691e-04,  1.9950e-04, -5.4304e-04, -1.1289e-04,
          3.6477e-04, -7.8721e-05,  4.5816e-04,  2.2927e-04,  7.7248e-05,
         -4.6424e-04,  2.9997e-04, -3.1925e-04,  4.4038e-04,  9.8836e-04,
         -6.1256e-05,  4.1107e-04, -7.4884e-05,  1.9906e-04, -8.0147e-04,
         -2.7036e-04,  2.2549e-04,  1.3975e-04, -1.2358e-04,  7.9689e-05,
          2.4147e-04, -1.0806e-04, -1.2486e-04,  6.6089e-05, -1.0361e-04,
          3.0422e-04,  6.7692e-04, -1.1023e-04, -5.4576e-04,  1.5624e-05,
         -4.9483e-04, -3.2199e-04,  3.6474e-05,  7.0545e-04, -3.2701e-04,
         -4.9202e-04, -3.2129e-04,  6.9875e-04, -2.0943e-04, -1.2256e-04,
          4.2960e-05,  1.2086e-04,  2.1602e-04, -1.0964e-05, -4.4924e-04,
         -5.7091e-04, -4.4125e-04,  5.0325e-04, -6.6549e-05, -3.7538e-05,
         -7.5262e-05, -5.5322e-04, -3.2662e-04,  4.9465e-04, -5.7139e-04,
         -2.4629e-04, -6.8254e-05,  5.0416e-04,  3.0643e-04,  1.2217e-04,
         -1.1993e-06,  4.1181e-04, -2.4064e-04,  1.4749e-04,  3.0985e-04,
          3.0473e-04,  1.9268e-04, -1.8850e-04, -5.5694e-04, -9.8527e-05,
         -3.9503e-04,  3.7939e-04, -2.6325e-05,  1.6691e-04,  1.1594e-04,
          2.7434e-04,  1.4533e-04, -5.5754e-05,  2.1280e-04,  1.2993e-04,
          2.5989e-04, -1.1185e-04,  1.8547e-04, -4.3705e-04, -1.4872e-04,
          4.2104e-04,  1.7182e-04, -1.7182e-04, -7.2342e-05, -1.0777e-04,
         -2.7484e-04, -2.0343e-04, -5.0263e-04,  2.0270e-04, -2.7172e-04,
          5.7275e-04, -2.0123e-05,  1.1092e-04, -8.5597e-06,  2.9679e-04,
         -9.3011e-05, -1.7931e-04, -3.0558e-04,  4.5218e-04,  3.3485e-04,
         -4.1525e-04, -4.2513e-05, -2.6101e-04, -2.6028e-04,  4.6399e-05,
          1.3512e-04, -7.8003e-05, -4.7164e-04, -5.3783e-05, -6.1490e-04,
          1.7600e-04,  3.9564e-04,  5.2565e-04,  3.5581e-04, -6.2091e-05,
         -2.1808e-05, -7.7326e-05,  4.9335e-04,  3.3902e-04,  1.4406e-04,
          2.8322e-04, -1.3830e-04,  1.9626e-04, -7.9075e-04, -1.2863e-05,
          2.3647e-05, -3.7023e-04, -2.3940e-04, -3.3962e-04, -2.5323e-04,
          1.1835e-04,  3.0117e-04,  9.6190e-07,  5.5471e-04,  5.1750e-04,
         -2.9991e-04,  1.9690e-04,  1.8517e-04,  4.3038e-05,  7.7798e-04,
          5.1015e-04,  3.8910e-04,  2.4784e-04,  3.8673e-04,  3.4420e-04,
          5.2627e-04, -7.3298e-05,  3.8081e-04, -1.2851e-04, -5.5205e-05,
          2.1675e-04,  2.0878e-04, -6.5748e-04,  5.5965e-05,  2.0232e-04,
         -1.3985e-04, -1.5566e-04,  4.9340e-05,  3.2342e-04, -1.5862e-04,
          1.3051e-04,  6.6741e-05, -2.8803e-05,  3.7650e-04, -2.0495e-04,
          1.1150e-04, -3.1048e-04,  3.9028e-04, -4.0873e-05,  2.5313e-04,
          3.4811e-04,  4.0547e-04,  3.5584e-04,  4.2142e-04, -1.6964e-05,
         -3.6865e-04, -8.6563e-05, -5.7891e-04, -2.7404e-04, -8.3930e-05,
         -9.4178e-04,  8.8495e-05, -2.5820e-04,  2.0980e-04,  2.7537e-04,
         -4.5910e-05, -1.9729e-04,  9.3968e-05, -3.9135e-04,  1.3670e-04,
         -1.8219e-04, -3.7462e-05,  5.2962e-04, -1.4969e-04, -2.0233e-05]],
       device='cuda:0', grad_fn=<ViewBackward>)
probs_pred: tensor([[7.0285e-05, 1.8726e-04, 7.1801e-07, 1.3521e-05, 8.9281e-04, 8.8259e-08,
         1.8322e-07, 3.7445e-03, 7.4738e-06, 1.3923e-04, 3.7306e-06, 1.2112e-04,
         1.6039e-04, 2.4251e-05, 1.4944e-06, 7.0997e-05, 4.6799e-05, 1.0158e-06,
         5.6879e-06, 2.3893e-04, 1.9038e-04, 2.0209e-05, 6.1241e-06, 1.4046e-04,
         2.4077e-06, 1.4159e-02, 2.6170e-06, 1.2005e-04, 2.1505e-05, 4.3948e-07,
         1.7688e-06, 7.4192e-06, 2.3719e-03, 1.0191e-05, 2.7607e-05, 2.1420e-06,
         1.8317e-05, 3.1260e-04, 4.5101e-04, 7.1004e-06, 8.9138e-05, 5.9155e-07,
         3.0505e-03, 2.1626e-05, 2.4748e-05, 4.5057e-05, 6.8584e-06, 1.3540e-05,
         9.5340e-05, 9.0110e-04, 3.3962e-07, 1.0783e-07, 2.6206e-02, 5.3081e-03,
         2.9601e-06, 1.3837e-04, 4.7008e-04, 2.4845e-06, 3.2894e-05, 7.9926e-07,
         5.6255e-05, 7.3298e-07, 1.3871e-02, 1.3071e-04, 2.0790e-05, 4.0258e-04,
         4.3568e-05, 6.4818e-06, 7.5393e-06, 2.0757e-04, 6.8549e-06, 1.2996e-05,
         3.8328e-03, 1.2615e-03, 4.1124e-03, 5.9025e-04, 8.5863e-06, 7.0695e-06,
         4.5473e-05, 5.0892e-06, 4.2415e-07, 9.7676e-07, 3.3331e-06, 2.1020e-06,
         4.0330e-05, 1.4879e-05, 2.5181e-06, 9.7902e-05, 1.4828e-04, 2.3473e-05,
         7.5051e-04, 3.0263e-05, 1.3835e-04, 1.5508e-05, 4.6551e-06, 3.9808e-04,
         8.3177e-04, 2.3263e-04, 1.3862e-07, 1.0232e-05, 1.2146e-03, 1.4400e-05,
         3.0903e-03, 3.1329e-04, 6.8506e-05, 3.0483e-07, 6.3533e-04, 1.2995e-06,
         2.5870e-03, 6.2038e-01, 1.7148e-05, 1.9298e-03, 1.4963e-05, 2.3162e-04,
         1.0460e-08, 2.1188e-06, 3.0167e-04, 1.2799e-04, 9.1951e-06, 7.0199e-05,
         3.5394e-04, 1.0739e-05, 9.0777e-06, 6.1273e-05, 1.1227e-05, 6.6294e-04,
         2.7548e-02, 1.0508e-05, 1.3491e-07, 3.6991e-05, 2.2450e-07, 1.2643e-06,
         4.5567e-05, 3.6643e-02, 1.2024e-06, 2.3089e-07, 1.2733e-06, 3.4267e-02,
         3.8969e-06, 9.2894e-06, 4.8621e-05, 1.0596e-04, 2.7442e-04, 2.8355e-05,
         3.5418e-07, 1.0491e-07, 3.8366e-07, 4.8511e-03, 1.6264e-05, 2.1738e-05,
         1.4907e-05, 1.2522e-07, 1.2071e-06, 4.4514e-03, 1.0440e-07, 2.6954e-06,
         1.5989e-05, 4.8954e-03, 6.7772e-04, 1.0735e-04, 3.1264e-05, 1.9441e-03,
         2.8520e-06, 1.3829e-04, 7.0128e-04, 6.6629e-04, 2.1730e-04, 4.8039e-06,
         1.2064e-07, 1.1813e-05, 6.0907e-07, 1.4058e-03, 2.4317e-05, 1.6793e-04,
         1.0087e-04, 4.9167e-04, 1.3534e-04, 1.8118e-05, 2.6573e-04, 1.1602e-04,
         4.2552e-04, 1.0340e-05, 2.0218e-04, 4.0009e-07, 7.1513e-06, 2.1321e-03,
         1.7638e-04, 5.6762e-06, 1.5349e-05, 1.0769e-05, 2.0260e-06, 4.1377e-06,
         2.0767e-07, 2.4020e-04, 2.0901e-06, 9.7199e-03, 2.5873e-05, 9.5928e-05,
         2.9045e-05, 6.1547e-04, 1.2483e-05, 5.2663e-06, 1.4899e-06, 2.9110e-03,
         9.0054e-04, 4.9754e-07, 2.0683e-05, 2.3264e-06, 2.3436e-06, 5.0322e-05,
         1.2219e-04, 1.4504e-05, 2.8311e-07, 1.8479e-05, 6.7570e-08, 1.8391e-04,
         1.6538e-03, 6.0690e-03, 1.1104e-03, 1.7005e-05, 2.5441e-05, 1.4602e-05,
         4.3936e-03, 9.3889e-04, 1.3363e-04, 5.3733e-04, 7.9366e-06, 2.2520e-04,
         1.1643e-08, 2.7822e-05, 4.0082e-05, 7.8047e-07, 2.8877e-06, 1.0600e-06,
         2.5146e-06, 1.0333e-04, 6.4303e-04, 3.1947e-05, 8.1156e-03, 5.5938e-03,
         1.5767e-06, 2.2666e-04, 2.0158e-04, 4.8658e-05, 7.5675e-02, 5.1977e-03,
         1.5491e-03, 3.7721e-04, 1.5128e-03, 9.8879e-04, 6.1067e-03, 1.5203e-05,
         1.4259e-03, 8.7525e-06, 1.8218e-05, 2.7643e-04, 2.5526e-04, 4.4143e-08,
         5.5373e-05, 2.3928e-04, 7.8145e-06, 6.6714e-06, 5.1824e-05, 8.0324e-04,
         6.4772e-06, 1.1670e-04, 6.1674e-05, 2.3722e-05, 1.3657e-03, 4.0752e-06,
         9.6491e-05, 1.4186e-06, 1.5675e-03, 2.1025e-05, 3.9771e-04, 1.0282e-03,
         1.8247e-03, 1.1108e-03, 2.1402e-03, 2.6704e-05, 7.9291e-07, 1.3314e-05,
         9.6843e-08, 2.0423e-06, 1.3669e-05, 2.5712e-09, 7.6661e-05, 2.3928e-06,
         2.5787e-04, 4.9678e-04, 1.9992e-05, 4.3998e-06, 8.0974e-05, 6.3191e-07,
         1.2415e-04, 5.1168e-06, 2.1755e-05, 6.3148e-03, 7.0821e-06, 2.5845e-05]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
labels: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
logits_pred: tensor([[-7.2732e-04,  1.5624e-04, -4.3680e-06, -1.8451e-03,  9.3429e-06,
         -2.3697e-03, -1.6876e-03,  3.4028e-05, -6.0513e-04, -9.9210e-04,
          1.2931e-03,  1.5543e-03, -7.3131e-04, -1.3993e-04,  3.8607e-04,
          5.8929e-05,  5.6436e-04,  6.8158e-04,  1.4249e-03, -1.4528e-03,
         -1.0822e-04,  1.1907e-03,  2.0303e-04,  6.7112e-04, -7.5896e-04,
          2.0728e-04, -9.8924e-05,  4.1760e-04, -7.0998e-04, -3.3997e-04,
         -8.2349e-04,  1.1504e-04, -6.8775e-04,  2.3203e-04,  7.8923e-04,
         -2.1937e-04,  1.3343e-03,  1.1277e-03,  7.3316e-04, -7.9139e-04,
         -6.3379e-04, -7.3204e-04, -3.8950e-04,  1.7773e-04,  9.8361e-04,
         -4.4680e-04, -3.9601e-04, -7.0557e-04,  1.0876e-03, -7.0384e-04,
         -7.1187e-04, -1.5787e-03,  3.2188e-04, -1.3435e-03,  9.7931e-04,
         -5.1918e-04, -2.8017e-04,  1.1221e-04, -2.0556e-04,  3.0275e-04,
         -1.0122e-03,  1.0946e-03, -8.6043e-04, -1.5252e-03,  4.7428e-04,
          7.7128e-04,  9.3674e-04, -6.4462e-04,  6.3588e-04,  2.9955e-04,
          1.3067e-03, -8.0847e-04,  7.4949e-04,  1.3070e-04, -9.1398e-04,
          9.9109e-04, -1.7923e-04, -1.9008e-03, -1.0345e-03, -6.0594e-04,
         -2.3487e-04,  1.2582e-03,  4.9573e-04,  6.7357e-04, -9.3543e-04,
         -4.8378e-04, -8.1978e-04,  4.9434e-04, -2.0108e-04, -1.3995e-04,
         -3.1293e-04,  3.9836e-04, -8.9377e-04, -2.3607e-04, -1.5414e-04,
          1.0505e-05,  1.7793e-03,  6.5392e-04, -4.8191e-04,  1.3782e-05,
         -7.5907e-05,  8.7080e-05,  6.5600e-04, -9.3481e-04,  2.2810e-04,
         -1.5079e-04, -2.9914e-04, -5.0814e-04, -8.8991e-04, -5.5331e-04,
         -1.0735e-03, -9.0445e-06, -6.3264e-04, -3.3703e-05, -4.0764e-05,
          5.3884e-04,  2.3173e-05,  5.1233e-04, -4.7528e-04, -1.1881e-04,
          9.5878e-04, -1.7826e-04,  3.9574e-04,  9.2532e-05, -1.0784e-03,
         -7.5783e-04,  7.5461e-05,  1.4267e-04, -5.6031e-04, -1.2684e-03,
         -1.1076e-03, -9.1329e-04,  1.7315e-03, -7.2780e-04,  3.3242e-04,
          2.2995e-04,  1.2657e-03, -8.2173e-04,  3.0791e-04,  1.8165e-04,
         -2.3048e-04,  1.9902e-03, -1.1762e-03, -2.3393e-04, -1.2514e-04,
         -1.4193e-03, -4.3492e-04, -1.0089e-03,  3.3269e-04,  2.4585e-05,
          7.1559e-04, -5.5263e-04,  7.4693e-05, -4.1429e-04,  7.4758e-04,
          1.9686e-04, -9.1354e-04, -1.3656e-03, -3.1741e-04, -1.4016e-04,
          5.1571e-05,  1.6133e-03,  7.1905e-04, -7.7486e-04,  2.2720e-05,
         -2.2306e-04,  1.1961e-03, -1.3761e-04,  1.0433e-04, -6.7152e-06,
          6.3692e-04,  8.6486e-05,  2.2071e-04,  1.2681e-03,  4.8021e-04,
         -6.4935e-05,  6.9516e-04, -1.0305e-03, -8.3291e-04,  3.0310e-04,
         -8.2435e-04, -9.2305e-04,  7.8325e-04, -5.6965e-04, -2.4889e-04,
         -1.2388e-04, -4.6826e-05,  1.7357e-05, -1.7381e-04, -7.7838e-04,
         -1.8283e-03,  2.1748e-04,  2.3176e-04,  7.1035e-05,  6.9028e-04,
          1.8139e-03,  1.2118e-03, -1.5211e-03, -2.7211e-04,  1.0212e-03,
          1.3164e-03,  4.5095e-04, -1.4300e-03,  9.4733e-05,  5.9734e-04,
          1.3462e-04, -2.0964e-03,  8.8108e-04, -7.1324e-05, -4.1632e-04,
          5.0785e-04,  3.4797e-04,  3.2597e-04, -1.8524e-05,  1.3847e-04,
          9.4689e-04,  7.3790e-04, -1.4578e-04,  1.4909e-03,  1.5471e-04,
          3.5300e-05,  1.4659e-03,  3.0067e-05, -6.5711e-04,  1.6741e-04,
          9.0078e-04, -7.7137e-04,  6.0081e-04, -1.2819e-03,  1.1477e-03,
         -6.1837e-04, -2.7885e-05,  1.0506e-04,  8.1729e-04,  4.5831e-04,
          1.6332e-03, -4.6820e-04,  8.8323e-04, -9.8187e-04,  1.5851e-04,
          1.1781e-04, -8.4427e-05,  4.5217e-04, -1.2952e-03, -7.6377e-04,
         -2.2052e-04,  4.1683e-04, -3.0351e-04, -1.2808e-03, -4.8749e-04,
         -3.0014e-04,  4.8853e-04,  7.2912e-04, -8.4874e-04, -6.8687e-04,
         -1.7049e-04,  3.6520e-05,  9.0913e-04,  8.8376e-05,  9.1717e-06,
         -2.6616e-04, -3.0139e-04, -3.8651e-04, -1.8734e-04, -1.3780e-03,
          1.3725e-03, -3.2192e-04, -8.8268e-04,  6.0204e-04, -1.5076e-03,
          1.5898e-04,  1.1148e-04,  9.0782e-04, -4.2212e-04, -1.3477e-03,
          6.0823e-04,  4.4653e-04, -1.2503e-03, -8.1534e-04, -3.2045e-04,
          1.7930e-04,  9.6935e-04,  7.8941e-04, -2.7763e-04,  3.1364e-04,
         -2.1399e-04, -1.2067e-03,  9.6381e-04,  9.5811e-04,  2.3877e-04,
          2.9770e-04, -1.2660e-03,  1.0308e-03, -3.6859e-04, -1.6060e-04,
         -4.6950e-04,  1.1129e-03, -3.1009e-04, -6.1178e-04, -4.4625e-04]],
       device='cuda:0', grad_fn=<ViewBackward>)
start_positions: tensor([90], device='cuda:0')
end_positions: tensor([95], device='cuda:0')
start_positions: tensor([17], device='cuda:0')
end_positions: tensor([17], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([43], device='cuda:0')
end_positions: tensor([43], device='cuda:0')
start_positions: tensor([163], device='cuda:0')
end_positions: tensor([165], device='cuda:0')
start_positions: tensor([36], device='cuda:0')
end_positions: tensor([38], device='cuda:0')
start_positions: tensor([52], device='cuda:0')
end_positions: tensor([53], device='cuda:0')
start_positions: tensor([36], device='cuda:0')
end_positions: tensor([46], device='cuda:0')
start_positions: tensor([45], device='cuda:0')
end_positions: tensor([46], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([247], device='cuda:0')
end_positions: tensor([252], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([79], device='cuda:0')
end_positions: tensor([84], device='cuda:0')
start_positions: tensor([37], device='cuda:0')
end_positions: tensor([48], device='cuda:0')
start_positions: tensor([26], device='cuda:0')
end_positions: tensor([27], device='cuda:0')
start_positions: tensor([92], device='cuda:0')
end_positions: tensor([100], device='cuda:0')
start_positions: tensor([102], device='cuda:0')
end_positions: tensor([103], device='cuda:0')
start_positions: tensor([191], device='cuda:0')
end_positions: tensor([193], device='cuda:0')
start_positions: tensor([0], device='cuda:0')
end_positions: tensor([0], device='cuda:0')
start_positions: tensor([149], device='cuda:0')05/20/2019 18:59:19 - INFO - __main__ -   epoch 1 step 109 train loss:0.6480353474617004
05/20/2019 18:59:27 - INFO - __main__ -   epoch 1 step 209 train loss:0.6777216792106628
05/20/2019 18:59:35 - INFO - __main__ -   epoch 1 step 309 train loss:0.5225255489349365
05/20/2019 18:59:43 - INFO - __main__ -   epoch 1 step 409 train loss:0.5647233128547668
05/20/2019 18:59:51 - INFO - __main__ -   epoch 1 step 509 train loss:0.4774456024169922
05/20/2019 19:00:00 - INFO - __main__ -   epoch 1 step 609 train loss:0.37871795892715454
05/20/2019 19:00:08 - INFO - __main__ -   epoch 1 step 709 train loss:0.37074753642082214
05/20/2019 19:00:16 - INFO - __main__ -   epoch 1 step 809 train loss:0.4123060405254364
05/20/2019 19:00:24 - INFO - __main__ -   epoch 1 step 909 train loss:0.3518661856651306
05/20/2019 19:00:32 - INFO - __main__ -   epoch 1 step 1009 train loss:0.39826980233192444
05/20/2019 19:00:41 - INFO - __main__ -   epoch 1 step 1109 train loss:0.29939544200897217
05/20/2019 19:00:49 - INFO - __main__ -   epoch 1 step 1209 train loss:0.27325358986854553
05/20/2019 19:00:57 - INFO - __main__ -   epoch 1 step 1309 train loss:0.1898084431886673
05/20/2019 19:01:05 - INFO - __main__ -   epoch 1 step 1409 train loss:0.22605781257152557
05/20/2019 19:01:13 - INFO - __main__ -   epoch 1 step 1509 train loss:0.23969760537147522
05/20/2019 19:01:21 - INFO - __main__ -   epoch 1 step 1609 train loss:0.18039079010486603
05/20/2019 19:01:29 - INFO - __main__ -   epoch 1 step 1709 train loss:0.24448604881763458
05/20/2019 19:01:38 - INFO - __main__ -   epoch 1 step 1809 train loss:0.20922163128852844
05/20/2019 19:01:46 - INFO - __main__ -   epoch 1 step 1909 train loss:0.23247389495372772
05/20/2019 19:01:54 - INFO - __main__ -   epoch 1 step 2009 train loss:0.1319628655910492
05/20/2019 19:02:02 - INFO - __main__ -   epoch 1 step 2109 train loss:0.2192855328321457
05/20/2019 19:02:10 - INFO - __main__ -   epoch 1 step 2209 train loss:0.20251865684986115
05/20/2019 19:02:18 - INFO - __main__ -   epoch 1 step 2309 train loss:0.196833536028862
05/20/2019 19:02:26 - INFO - __main__ -   epoch 1 step 2409 train loss:0.20125074684619904
05/20/2019 19:02:35 - INFO - __main__ -   epoch 1 step 2509 train loss:0.17746983468532562
05/20/2019 19:02:43 - INFO - __main__ -   epoch 1 step 2609 train loss:0.19411632418632507
05/20/2019 19:02:51 - INFO - __main__ -   epoch 1 step 2709 train loss:0.20765213668346405
05/20/2019 19:02:59 - INFO - __main__ -   epoch 1 step 2809 train loss:0.17004834115505219
05/20/2019 19:03:07 - INFO - __main__ -   epoch 1 step 2909 train loss:0.24855796992778778
05/20/2019 19:03:14 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 19:08:45 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_3000_predictions.json
05/20/2019 19:08:45 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_3000_nbest_predictions.json
05/20/2019 19:09:46 - INFO - __main__ -   start evaluation script.................
05/20/2019 19:09:48 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 19:09:48 - INFO - __main__ -   epoch 1 step 3000 eval_loss: 3.5389339923858643 evaluate f1: 4.115022154372557 evaluate best f1:50.07159100480081
05/20/2019 19:09:49 - INFO - __main__ -   epoch 1 step 3009 train loss:0.08722101151943207
05/20/2019 19:09:56 - INFO - __main__ -   epoch 1 step 3109 train loss:0.1895211786031723
05/20/2019 19:10:04 - INFO - __main__ -   epoch 1 step 3209 train loss:0.14290396869182587
05/20/2019 19:10:12 - INFO - __main__ -   epoch 1 step 3309 train loss:0.18752920627593994
05/20/2019 19:10:19 - INFO - __main__ -   epoch 1 step 3409 train loss:0.22373567521572113
05/20/2019 19:10:27 - INFO - __main__ -   epoch 1 step 3509 train loss:0.2546766698360443
05/20/2019 19:10:35 - INFO - __main__ -   epoch 1 step 3609 train loss:0.16416820883750916
05/20/2019 19:10:43 - INFO - __main__ -   epoch 1 step 3709 train loss:0.1638333648443222
05/20/2019 19:10:51 - INFO - __main__ -   epoch 1 step 3809 train loss:0.19503237307071686
05/20/2019 19:10:58 - INFO - __main__ -   epoch 1 step 3909 train loss:0.17590463161468506
05/20/2019 19:11:06 - INFO - __main__ -   epoch 1 step 4009 train loss:0.17384760081768036
05/20/2019 19:11:14 - INFO - __main__ -   epoch 1 step 4109 train loss:0.12852703034877777
05/20/2019 19:11:22 - INFO - __main__ -   epoch 1 step 4209 train loss:0.19971176981925964
05/20/2019 19:11:30 - INFO - __main__ -   epoch 1 step 4309 train loss:0.23667564988136292
05/20/2019 19:11:38 - INFO - __main__ -   epoch 1 step 4409 train loss:0.14979702234268188
05/20/2019 19:11:45 - INFO - __main__ -   epoch 1 step 4509 train loss:0.2020261585712433
05/20/2019 19:11:53 - INFO - __main__ -   epoch 1 step 4609 train loss:0.21937935054302216
05/20/2019 19:12:01 - INFO - __main__ -   epoch 1 step 4709 train loss:0.14060088992118835
05/20/2019 19:12:09 - INFO - __main__ -   epoch 1 step 4809 train loss:0.22875194251537323
05/20/2019 19:12:17 - INFO - __main__ -   epoch 1 step 4909 train loss:0.1780715137720108
05/20/2019 19:12:25 - INFO - __main__ -   epoch 1 step 5009 train loss:0.1938941478729248
05/20/2019 19:12:32 - INFO - __main__ -   epoch 1 step 5109 train loss:0.20252059400081635
05/20/2019 19:12:40 - INFO - __main__ -   epoch 1 step 5209 train loss:0.13871048390865326
05/20/2019 19:12:48 - INFO - __main__ -   epoch 1 step 5309 train loss:0.16438691318035126
05/20/2019 19:12:56 - INFO - __main__ -   epoch 1 step 5409 train loss:0.17634208500385284
05/20/2019 19:13:04 - INFO - __main__ -   epoch 1 step 5509 train loss:0.18527595698833466
05/20/2019 19:13:12 - INFO - __main__ -   epoch 1 step 5609 train loss:0.17147196829319
05/20/2019 19:13:19 - INFO - __main__ -   epoch 1 step 5709 train loss:0.14763836562633514
05/20/2019 19:13:27 - INFO - __main__ -   epoch 1 step 5809 train loss:0.22855928540229797
05/20/2019 19:13:35 - INFO - __main__ -   epoch 1 step 5909 train loss:0.17291836440563202
05/20/2019 19:13:42 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 19:19:13 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_6000_predictions.json
05/20/2019 19:19:13 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_6000_nbest_predictions.json
05/20/2019 19:20:16 - INFO - __main__ -   start evaluation script.................
05/20/2019 19:20:18 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 19:20:18 - INFO - __main__ -   epoch 1 step 6000 eval_loss: 3.4693779945373535 evaluate f1: 4.353642592605999 evaluate best f1:50.07159100480081
05/20/2019 19:20:19 - INFO - __main__ -   epoch 1 step 6009 train loss:0.20338664948940277
05/20/2019 19:20:26 - INFO - __main__ -   epoch 1 step 6109 train loss:0.21709826588630676
05/20/2019 19:20:34 - INFO - __main__ -   epoch 1 step 6209 train loss:0.23439787328243256
05/20/2019 19:20:42 - INFO - __main__ -   epoch 1 step 6309 train loss:0.18697844445705414
05/20/2019 19:20:50 - INFO - __main__ -   epoch 1 step 6409 train loss:0.17866352200508118
05/20/2019 19:20:58 - INFO - __main__ -   epoch 1 step 6509 train loss:0.20269449055194855
05/20/2019 19:21:05 - INFO - __main__ -   epoch 1 step 6609 train loss:0.2036478966474533
05/20/2019 19:21:13 - INFO - __main__ -   epoch 1 step 6709 train loss:0.20837818086147308
05/20/2019 19:21:21 - INFO - __main__ -   epoch 1 step 6809 train loss:0.1906183362007141
05/20/2019 19:21:29 - INFO - __main__ -   epoch 1 step 6909 train loss:0.17411428689956665
05/20/2019 19:21:37 - INFO - __main__ -   epoch 1 step 7009 train loss:0.1329246610403061
05/20/2019 19:21:45 - INFO - __main__ -   epoch 1 step 7109 train loss:0.12949779629707336
05/20/2019 19:21:52 - INFO - __main__ -   epoch 1 step 7209 train loss:0.16780345141887665
05/20/2019 19:22:00 - INFO - __main__ -   epoch 1 step 7309 train loss:0.18151305615901947
05/20/2019 19:22:08 - INFO - __main__ -   epoch 1 step 7409 train loss:0.1534738689661026
05/20/2019 19:22:16 - INFO - __main__ -   epoch 1 step 7509 train loss:0.22381459176540375
05/20/2019 19:22:24 - INFO - __main__ -   epoch 1 step 7609 train loss:0.14198267459869385
05/20/2019 19:22:31 - INFO - __main__ -   epoch 1 step 7709 train loss:0.19016152620315552
05/20/2019 19:22:39 - INFO - __main__ -   epoch 1 step 7809 train loss:0.14243647456169128
05/20/2019 19:22:47 - INFO - __main__ -   epoch 1 step 7909 train loss:0.23292212188243866
05/20/2019 19:22:54 - INFO - __main__ -   epoch 1 step 8009 train loss:0.18458132445812225
05/20/2019 19:23:02 - INFO - __main__ -   epoch 1 step 8109 train loss:0.1849735677242279
05/20/2019 19:23:10 - INFO - __main__ -   epoch 1 step 8209 train loss:0.13374251127243042
05/20/2019 19:23:18 - INFO - __main__ -   epoch 1 step 8309 train loss:0.188137024641037
05/20/2019 19:23:25 - INFO - __main__ -   epoch 1 step 8409 train loss:0.1759636104106903
05/20/2019 19:23:33 - INFO - __main__ -   epoch 1 step 8509 train loss:0.1928737312555313
05/20/2019 19:23:41 - INFO - __main__ -   epoch 1 step 8609 train loss:0.1833663433790207
05/20/2019 19:23:49 - INFO - __main__ -   epoch 1 step 8709 train loss:0.16659756004810333
05/20/2019 19:23:56 - INFO - __main__ -   epoch 1 step 8809 train loss:0.22691218554973602
05/20/2019 19:24:04 - INFO - __main__ -   epoch 1 step 8909 train loss:0.1494612991809845
05/20/2019 19:24:11 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 19:29:42 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_9000_predictions.json
05/20/2019 19:29:42 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_9000_nbest_predictions.json
05/20/2019 19:30:48 - INFO - __main__ -   start evaluation script.................
05/20/2019 19:30:51 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 19:30:51 - INFO - __main__ -   epoch 1 step 9000 eval_loss: 3.495898723602295 evaluate f1: 4.397795230376927 evaluate best f1:50.07159100480081
05/20/2019 19:30:52 - INFO - __main__ -   epoch 1 step 9009 train loss:0.18579256534576416
05/20/2019 19:30:59 - INFO - __main__ -   epoch 1 step 9109 train loss:0.18168561160564423
05/20/2019 19:31:07 - INFO - __main__ -   epoch 1 step 9209 train loss:0.1939622312784195
05/20/2019 19:31:15 - INFO - __main__ -   epoch 1 step 9309 train loss:0.17014871537685394
05/20/2019 19:31:23 - INFO - __main__ -   epoch 1 step 9409 train loss:0.21340210735797882
05/20/2019 19:31:30 - INFO - __main__ -   epoch 1 step 9509 train loss:0.1923941671848297
05/20/2019 19:31:38 - INFO - __main__ -   epoch 1 step 9609 train loss:0.20693548023700714
05/20/2019 19:31:46 - INFO - __main__ -   epoch 1 step 9709 train loss:0.18538251519203186
05/20/2019 19:31:53 - INFO - __main__ -   epoch 1 step 9809 train loss:0.19282987713813782
05/20/2019 19:32:01 - INFO - __main__ -   epoch 1 step 9909 train loss:0.18058395385742188
05/20/2019 19:32:09 - INFO - __main__ -   epoch 1 step 10009 train loss:0.2010401338338852
05/20/2019 19:32:17 - INFO - __main__ -   epoch 1 step 10109 train loss:0.13877873122692108
05/20/2019 19:32:24 - INFO - __main__ -   epoch 1 step 10209 train loss:0.22050166130065918
05/20/2019 19:32:32 - INFO - __main__ -   epoch 1 step 10309 train loss:0.16451378166675568
05/20/2019 19:32:40 - INFO - __main__ -   epoch 1 step 10409 train loss:0.20903030037879944
05/20/2019 19:32:48 - INFO - __main__ -   epoch 1 step 10509 train loss:0.16352008283138275
05/20/2019 19:32:56 - INFO - __main__ -   epoch 1 step 10609 train loss:0.20271065831184387
05/20/2019 19:33:04 - INFO - __main__ -   epoch 1 step 10709 train loss:0.17993372678756714
05/20/2019 19:33:11 - INFO - __main__ -   epoch 1 step 10809 train loss:0.14725667238235474
05/20/2019 19:33:19 - INFO - __main__ -   epoch 1 step 10909 train loss:0.1808134764432907
05/20/2019 19:33:27 - INFO - __main__ -   epoch 1 step 11009 train loss:0.2098436802625656
05/20/2019 19:33:35 - INFO - __main__ -   epoch 1 step 11109 train loss:0.1604003757238388
05/20/2019 19:33:43 - INFO - __main__ -   epoch 1 step 11209 train loss:0.18891745805740356
05/20/2019 19:33:50 - INFO - __main__ -   epoch 1 step 11309 train loss:0.16388319432735443
05/20/2019 19:33:58 - INFO - __main__ -   epoch 1 step 11409 train loss:0.16292406618595123
05/20/2019 19:34:06 - INFO - __main__ -   epoch 1 step 11509 train loss:0.22463145852088928
05/20/2019 19:34:14 - INFO - __main__ -   epoch 1 step 11609 train loss:0.16107392311096191
05/20/2019 19:34:22 - INFO - __main__ -   epoch 1 step 11709 train loss:0.16903112828731537
05/20/2019 19:34:29 - INFO - __main__ -   epoch 1 step 11809 train loss:0.16412530839443207
05/20/2019 19:34:37 - INFO - __main__ -   epoch 1 step 11909 train loss:0.1803266406059265
05/20/2019 19:34:44 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 19:40:15 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_12000_predictions.json
05/20/2019 19:40:15 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_12000_nbest_predictions.json
05/20/2019 19:41:08 - INFO - __main__ -   start evaluation script.................
05/20/2019 19:41:09 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 19:41:09 - INFO - __main__ -   epoch 1 step 12000 eval_loss: 3.4406518936157227 evaluate f1: 3.470845908277781 evaluate best f1:50.07159100480081
05/20/2019 19:41:10 - INFO - __main__ -   epoch 1 step 12009 train loss:0.16194455325603485
05/20/2019 19:41:18 - INFO - __main__ -   epoch 1 step 12109 train loss:0.10435722023248672
05/20/2019 19:41:26 - INFO - __main__ -   epoch 1 step 12209 train loss:0.16076676547527313
05/20/2019 19:41:33 - INFO - __main__ -   epoch 1 step 12309 train loss:0.18954752385616302
05/20/2019 19:41:41 - INFO - __main__ -   epoch 1 step 12409 train loss:0.178645059466362
05/20/2019 19:41:49 - INFO - __main__ -   epoch 1 step 12509 train loss:0.19656157493591309
05/20/2019 19:41:57 - INFO - __main__ -   epoch 1 step 12609 train loss:0.17778950929641724
05/20/2019 19:42:05 - INFO - __main__ -   epoch 1 step 12709 train loss:0.13182206451892853
05/20/2019 19:42:13 - INFO - __main__ -   epoch 1 step 12809 train loss:0.13967202603816986
05/20/2019 19:42:20 - INFO - __main__ -   epoch 1 step 12909 train loss:0.14332976937294006
05/20/2019 19:42:28 - INFO - __main__ -   epoch 1 step 13009 train loss:0.17093969881534576
05/20/2019 19:42:36 - INFO - __main__ -   epoch 1 step 13109 train loss:0.21664415299892426
05/20/2019 19:42:44 - INFO - __main__ -   epoch 1 step 13209 train loss:0.23152990639209747
05/20/2019 19:42:51 - INFO - __main__ -   epoch 1 step 13309 train loss:0.14616569876670837
05/20/2019 19:42:59 - INFO - __main__ -   epoch 1 step 13409 train loss:0.16424645483493805
05/20/2019 19:43:07 - INFO - __main__ -   epoch 1 step 13509 train loss:0.20080800354480743
05/20/2019 19:43:15 - INFO - __main__ -   epoch 1 step 13609 train loss:0.2041729986667633
05/20/2019 19:43:23 - INFO - __main__ -   epoch 1 step 13709 train loss:0.1919313222169876
05/20/2019 19:43:31 - INFO - __main__ -   epoch 1 step 13809 train loss:0.18455760180950165
05/20/2019 19:43:39 - INFO - __main__ -   epoch 1 step 13909 train loss:0.14302001893520355
05/20/2019 19:43:46 - INFO - __main__ -   epoch 1 step 14009 train loss:0.19232217967510223
05/20/2019 19:43:54 - INFO - __main__ -   epoch 1 step 14109 train loss:0.14724388718605042
05/20/2019 19:44:02 - INFO - __main__ -   epoch 1 step 14209 train loss:0.1878199577331543
05/20/2019 19:44:10 - INFO - __main__ -   epoch 1 step 14309 train loss:0.15686561167240143
05/20/2019 19:44:18 - INFO - __main__ -   epoch 1 step 14409 train loss:0.2536434829235077
05/20/2019 19:44:26 - INFO - __main__ -   epoch 1 step 14509 train loss:0.1813424974679947
05/20/2019 19:44:33 - INFO - __main__ -   epoch 1 step 14609 train loss:0.20435944199562073
05/20/2019 19:44:41 - INFO - __main__ -   epoch 1 step 14709 train loss:0.11713888496160507
05/20/2019 19:44:49 - INFO - __main__ -   epoch 1 step 14809 train loss:0.1614183485507965
05/20/2019 19:44:57 - INFO - __main__ -   epoch 1 step 14909 train loss:0.18270598351955414
05/20/2019 19:45:04 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 19:50:41 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_15000_predictions.json
05/20/2019 19:50:41 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_15000_nbest_predictions.json
05/20/2019 19:51:45 - INFO - __main__ -   start evaluation script.................
05/20/2019 19:51:48 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 19:51:48 - INFO - __main__ -   epoch 1 step 15000 eval_loss: 3.2600350379943848 evaluate f1: 4.525627948159268 evaluate best f1:50.07580224037733
05/20/2019 19:51:49 - INFO - __main__ -   epoch 1 step 15009 train loss:0.18461698293685913
05/20/2019 19:51:56 - INFO - __main__ -   epoch 1 step 15109 train loss:0.16361573338508606
05/20/2019 19:52:04 - INFO - __main__ -   epoch 1 step 15209 train loss:0.15590602159500122
05/20/2019 19:52:12 - INFO - __main__ -   epoch 1 step 15309 train loss:0.12313192337751389
05/20/2019 19:52:20 - INFO - __main__ -   epoch 1 step 15409 train loss:0.1887580156326294
05/20/2019 19:52:27 - INFO - __main__ -   epoch 1 step 15509 train loss:0.1394355446100235
05/20/2019 19:52:35 - INFO - __main__ -   epoch 1 step 15609 train loss:0.1792134791612625
05/20/2019 19:52:43 - INFO - __main__ -   epoch 1 step 15709 train loss:0.2111126035451889
05/20/2019 19:52:51 - INFO - __main__ -   epoch 1 step 15809 train loss:0.16332963109016418
05/20/2019 19:52:58 - INFO - __main__ -   epoch 1 step 15909 train loss:0.14490652084350586
05/20/2019 19:53:06 - INFO - __main__ -   epoch 1 step 16009 train loss:0.1633518487215042
05/20/2019 19:53:14 - INFO - __main__ -   epoch 1 step 16109 train loss:0.16277974843978882
05/20/2019 19:53:22 - INFO - __main__ -   epoch 1 step 16209 train loss:0.1915595978498459
05/20/2019 19:53:30 - INFO - __main__ -   epoch 1 step 16309 train loss:0.14293517172336578
05/20/2019 19:53:37 - INFO - __main__ -   epoch 1 step 16409 train loss:0.17975962162017822
05/20/2019 19:53:45 - INFO - __main__ -   epoch 1 step 16509 train loss:0.1513623595237732
05/20/2019 19:53:53 - INFO - __main__ -   epoch 1 step 16609 train loss:0.18067866563796997
05/20/2019 19:54:01 - INFO - __main__ -   epoch 1 step 16709 train loss:0.18238650262355804
05/20/2019 19:54:09 - INFO - __main__ -   epoch 1 step 16809 train loss:0.1679508239030838
05/20/2019 19:54:17 - INFO - __main__ -   epoch 1 step 16909 train loss:0.18470822274684906
05/20/2019 19:54:25 - INFO - __main__ -   epoch 1 step 17009 train loss:0.15672700107097626
05/20/2019 19:54:32 - INFO - __main__ -   epoch 1 step 17109 train loss:0.16479967534542084
05/20/2019 19:54:40 - INFO - __main__ -   epoch 1 step 17209 train loss:0.15780285000801086
05/20/2019 19:54:48 - INFO - __main__ -   epoch 1 step 17309 train loss:0.1704786866903305
05/20/2019 19:54:56 - INFO - __main__ -   epoch 1 step 17409 train loss:0.11494342237710953
05/20/2019 19:55:04 - INFO - __main__ -   epoch 1 step 17509 train loss:0.15465690195560455
05/20/2019 19:55:12 - INFO - __main__ -   epoch 1 step 17609 train loss:0.1799759417772293
05/20/2019 19:55:20 - INFO - __main__ -   epoch 1 step 17709 train loss:0.13593894243240356
05/20/2019 19:55:27 - INFO - __main__ -   epoch 1 step 17809 train loss:0.2032289355993271
05/20/2019 19:55:35 - INFO - __main__ -   epoch 1 step 17909 train loss:0.21976175904273987
05/20/2019 19:55:43 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:01:15 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_18000_predictions.json
05/20/2019 20:01:15 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_18000_nbest_predictions.json
05/20/2019 20:02:12 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:02:15 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:02:15 - INFO - __main__ -   epoch 1 step 18000 eval_loss: 3.3862242698669434 evaluate f1: 4.237317040474212 evaluate best f1:50.07159100480081
05/20/2019 20:02:16 - INFO - __main__ -   epoch 1 step 18009 train loss:0.16985057294368744
05/20/2019 20:02:24 - INFO - __main__ -   epoch 1 step 18109 train loss:0.1640777289867401
05/20/2019 20:02:31 - INFO - __main__ -   epoch 1 step 18209 train loss:0.15152636170387268
05/20/2019 20:02:39 - INFO - __main__ -   epoch 1 step 18309 train loss:0.10177446901798248
05/20/2019 20:02:47 - INFO - __main__ -   epoch 1 step 18409 train loss:0.17952720820903778
05/20/2019 20:02:55 - INFO - __main__ -   epoch 1 step 18509 train loss:0.1840488612651825
05/20/2019 20:03:03 - INFO - __main__ -   epoch 1 step 18609 train loss:0.18074217438697815
05/20/2019 20:03:11 - INFO - __main__ -   epoch 1 step 18709 train loss:0.15351246297359467
05/20/2019 20:03:18 - INFO - __main__ -   epoch 1 step 18809 train loss:0.21985630691051483
05/20/2019 20:03:26 - INFO - __main__ -   epoch 1 step 18909 train loss:0.15953071415424347
05/20/2019 20:03:34 - INFO - __main__ -   epoch 1 step 19009 train loss:0.15701165795326233
05/20/2019 20:03:42 - INFO - __main__ -   epoch 1 step 19109 train loss:0.20065894722938538
05/20/2019 20:03:50 - INFO - __main__ -   epoch 1 step 19209 train loss:0.16714730858802795
05/20/2019 20:03:57 - INFO - __main__ -   epoch 1 step 19309 train loss:0.12901832163333893
05/20/2019 20:04:05 - INFO - __main__ -   epoch 1 step 19409 train loss:0.1808808296918869
05/20/2019 20:04:13 - INFO - __main__ -   epoch 1 step 19509 train loss:0.1430695503950119
05/20/2019 20:04:21 - INFO - __main__ -   epoch 1 step 19609 train loss:0.16798639297485352
05/20/2019 20:04:29 - INFO - __main__ -   epoch 1 step 19709 train loss:0.18569956719875336
05/20/2019 20:04:36 - INFO - __main__ -   epoch 1 step 19809 train loss:0.16069674491882324
05/20/2019 20:04:44 - INFO - __main__ -   epoch 1 step 19909 train loss:0.15753965079784393
05/20/2019 20:04:52 - INFO - __main__ -   epoch 1 step 20009 train loss:0.19449825584888458
05/20/2019 20:05:00 - INFO - __main__ -   epoch 1 step 20109 train loss:0.1477011889219284
05/20/2019 20:05:08 - INFO - __main__ -   epoch 1 step 20209 train loss:0.17572923004627228
05/20/2019 20:05:16 - INFO - __main__ -   epoch 1 step 20309 train loss:0.15003365278244019
05/20/2019 20:05:24 - INFO - __main__ -   epoch 1 step 20409 train loss:0.13333404064178467
05/20/2019 20:05:32 - INFO - __main__ -   epoch 1 step 20509 train loss:0.24330227077007294
05/20/2019 20:05:39 - INFO - __main__ -   epoch 1 step 20609 train loss:0.19479335844516754
05/20/2019 20:05:47 - INFO - __main__ -   epoch 1 step 20709 train loss:0.1622067093849182
05/20/2019 20:05:55 - INFO - __main__ -   epoch 1 step 20809 train loss:0.18460896611213684
05/20/2019 20:06:03 - INFO - __main__ -   epoch 1 step 20909 train loss:0.15143485367298126
05/20/2019 20:06:10 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:11:42 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_21000_predictions.json
05/20/2019 20:11:42 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_21000_nbest_predictions.json
05/20/2019 20:12:44 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:12:46 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:12:46 - INFO - __main__ -   epoch 1 step 21000 eval_loss: 3.20386004447937 evaluate f1: 4.539934184016628 evaluate best f1:50.07159100480081
05/20/2019 20:12:46 - INFO - __main__ -   epoch 1 step 21009 train loss:0.13322363793849945
05/20/2019 20:12:54 - INFO - __main__ -   epoch 1 step 21109 train loss:0.19428609311580658
05/20/2019 20:13:02 - INFO - __main__ -   epoch 1 step 21209 train loss:0.21658635139465332
05/20/2019 20:13:10 - INFO - __main__ -   epoch 1 step 21309 train loss:0.15437360107898712
05/20/2019 20:13:18 - INFO - __main__ -   epoch 1 step 21409 train loss:0.1591348946094513
05/20/2019 20:13:26 - INFO - __main__ -   epoch 1 step 21509 train loss:0.09565777331590652
05/20/2019 20:13:33 - INFO - __main__ -   epoch 1 step 21609 train loss:0.1301206797361374
05/20/2019 20:13:41 - INFO - __main__ -   epoch 1 step 21709 train loss:0.22986681759357452
05/20/2019 20:13:49 - INFO - __main__ -   epoch 1 step 21809 train loss:0.15117649734020233
05/20/2019 20:13:57 - INFO - __main__ -   epoch 1 step 21909 train loss:0.14651314914226532
05/20/2019 20:14:05 - INFO - __main__ -   epoch 1 step 22009 train loss:0.142450749874115
05/20/2019 20:14:13 - INFO - __main__ -   epoch 1 step 22109 train loss:0.1948404312133789
05/20/2019 20:14:21 - INFO - __main__ -   epoch 1 step 22209 train loss:0.11792855709791183
05/20/2019 20:14:29 - INFO - __main__ -   epoch 1 step 22309 train loss:0.134344220161438
05/20/2019 20:14:36 - INFO - __main__ -   epoch 1 step 22409 train loss:0.18033349514007568
05/20/2019 20:14:44 - INFO - __main__ -   epoch 1 step 22509 train loss:0.16771860420703888
05/20/2019 20:14:52 - INFO - __main__ -   epoch 1 step 22609 train loss:0.22937993705272675
05/20/2019 20:15:00 - INFO - __main__ -   epoch 1 step 22709 train loss:0.18369117379188538
05/20/2019 20:15:08 - INFO - __main__ -   epoch 1 step 22809 train loss:0.1889505237340927
05/20/2019 20:15:16 - INFO - __main__ -   epoch 1 step 22909 train loss:0.16157324612140656
05/20/2019 20:15:24 - INFO - __main__ -   epoch 1 step 23009 train loss:0.17482328414916992
05/20/2019 20:15:31 - INFO - __main__ -   epoch 1 step 23109 train loss:0.10712853819131851
05/20/2019 20:15:39 - INFO - __main__ -   epoch 1 step 23209 train loss:0.1602211743593216
05/20/2019 20:15:47 - INFO - __main__ -   epoch 1 step 23309 train loss:0.11695542186498642
05/20/2019 20:15:55 - INFO - __main__ -   epoch 1 step 23409 train loss:0.12314653396606445
05/20/2019 20:16:03 - INFO - __main__ -   epoch 1 step 23509 train loss:0.17843441665172577
05/20/2019 20:16:10 - INFO - __main__ -   epoch 1 step 23609 train loss:0.11049144715070724
05/20/2019 20:16:18 - INFO - __main__ -   epoch 1 step 23709 train loss:0.18281856179237366
05/20/2019 20:16:26 - INFO - __main__ -   epoch 1 step 23809 train loss:0.17557935416698456
05/20/2019 20:16:34 - INFO - __main__ -   epoch 1 step 23909 train loss:0.18227891623973846
05/20/2019 20:16:41 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:22:16 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_24000_predictions.json
05/20/2019 20:22:16 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_24000_nbest_predictions.json
05/20/2019 20:23:09 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:23:12 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:23:12 - INFO - __main__ -   epoch 1 step 24000 eval_loss: 3.2323265075683594 evaluate f1: 5.013999109495036 evaluate best f1:50.07327549903142
05/20/2019 20:23:13 - INFO - __main__ -   epoch 1 step 24009 train loss:0.14159196615219116
05/20/2019 20:23:20 - INFO - __main__ -   epoch 1 step 24109 train loss:0.1633765548467636
05/20/2019 20:23:28 - INFO - __main__ -   epoch 1 step 24209 train loss:0.11148016899824142
05/20/2019 20:23:36 - INFO - __main__ -   epoch 1 step 24309 train loss:0.1372259110212326
05/20/2019 20:23:44 - INFO - __main__ -   epoch 1 step 24409 train loss:0.19181640446186066
05/20/2019 20:23:52 - INFO - __main__ -   epoch 1 step 24509 train loss:0.1285024732351303
05/20/2019 20:24:00 - INFO - __main__ -   epoch 1 step 24609 train loss:0.1281522959470749
05/20/2019 20:24:08 - INFO - __main__ -   epoch 1 step 24709 train loss:0.18134166300296783
05/20/2019 20:24:15 - INFO - __main__ -   epoch 1 step 24809 train loss:0.16830052435398102
05/20/2019 20:24:23 - INFO - __main__ -   epoch 1 step 24909 train loss:0.15858153998851776
05/20/2019 20:24:31 - INFO - __main__ -   epoch 1 step 25009 train loss:0.18919824063777924
05/20/2019 20:24:39 - INFO - __main__ -   epoch 1 step 25109 train loss:0.19353078305721283
05/20/2019 20:24:47 - INFO - __main__ -   epoch 1 step 25209 train loss:0.16268962621688843
05/20/2019 20:24:55 - INFO - __main__ -   epoch 1 step 25309 train loss:0.16765809059143066
05/20/2019 20:25:03 - INFO - __main__ -   epoch 1 step 25409 train loss:0.14355194568634033
05/20/2019 20:25:11 - INFO - __main__ -   epoch 1 step 25509 train loss:0.12893830239772797
05/20/2019 20:25:18 - INFO - __main__ -   epoch 1 step 25609 train loss:0.17173638939857483
05/20/2019 20:25:26 - INFO - __main__ -   epoch 1 step 25709 train loss:0.17075087130069733
05/20/2019 20:25:34 - INFO - __main__ -   epoch 1 step 25809 train loss:0.15973539650440216
05/20/2019 20:25:42 - INFO - __main__ -   epoch 1 step 25909 train loss:0.1882273405790329
05/20/2019 20:25:50 - INFO - __main__ -   epoch 1 step 26009 train loss:0.1832786500453949
05/20/2019 20:25:58 - INFO - __main__ -   epoch 1 step 26109 train loss:0.1551467627286911
05/20/2019 20:26:06 - INFO - __main__ -   epoch 1 step 26209 train loss:0.1743166148662567
05/20/2019 20:26:13 - INFO - __main__ -   epoch 1 step 26309 train loss:0.16587957739830017
05/20/2019 20:26:21 - INFO - __main__ -   epoch 1 step 26409 train loss:0.1278398036956787
05/20/2019 20:26:29 - INFO - __main__ -   epoch 1 step 26509 train loss:0.11390813440084457
05/20/2019 20:26:37 - INFO - __main__ -   epoch 1 step 26609 train loss:0.17396192252635956
05/20/2019 20:26:45 - INFO - __main__ -   epoch 1 step 26709 train loss:0.20644770562648773
05/20/2019 20:26:53 - INFO - __main__ -   epoch 1 step 26809 train loss:0.20123587548732758
05/20/2019 20:27:00 - INFO - __main__ -   epoch 1 step 26909 train loss:0.10552897304296494
05/20/2019 20:27:08 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:32:41 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_27000_predictions.json
05/20/2019 20:32:41 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_27000_nbest_predictions.json
05/20/2019 20:33:39 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:33:42 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:33:42 - INFO - __main__ -   epoch 1 step 27000 eval_loss: 3.1623899936676025 evaluate f1: 4.084719841797927 evaluate best f1:50.07159100480081
05/20/2019 20:33:42 - INFO - __main__ -   epoch 1 step 27009 train loss:0.19967956840991974
05/20/2019 20:33:50 - INFO - __main__ -   epoch 1 step 27109 train loss:0.20171038806438446
05/20/2019 20:33:58 - INFO - __main__ -   epoch 1 step 27209 train loss:0.1710248738527298
05/20/2019 20:34:06 - INFO - __main__ -   epoch 1 step 27309 train loss:0.20062533020973206
05/20/2019 20:34:14 - INFO - __main__ -   epoch 1 step 27409 train loss:0.1755775809288025
05/20/2019 20:34:21 - INFO - __main__ -   epoch 1 step 27509 train loss:0.16495469212532043
05/20/2019 20:34:29 - INFO - __main__ -   epoch 1 step 27609 train loss:0.2513722777366638
05/20/2019 20:34:37 - INFO - __main__ -   epoch 1 step 27709 train loss:0.20516042411327362
05/20/2019 20:34:45 - INFO - __main__ -   epoch 1 step 27809 train loss:0.1589798778295517
05/20/2019 20:34:53 - INFO - __main__ -   epoch 1 step 27909 train loss:0.07145781815052032
05/20/2019 20:35:01 - INFO - __main__ -   epoch 1 step 28009 train loss:0.17357061803340912
05/20/2019 20:35:09 - INFO - __main__ -   epoch 1 step 28109 train loss:0.2012653350830078
05/20/2019 20:35:17 - INFO - __main__ -   epoch 1 step 28209 train loss:0.10982915014028549
05/20/2019 20:35:24 - INFO - __main__ -   epoch 1 step 28309 train loss:0.10124517977237701
05/20/2019 20:35:32 - INFO - __main__ -   epoch 1 step 28409 train loss:0.12911540269851685
05/20/2019 20:35:40 - INFO - __main__ -   epoch 1 step 28509 train loss:0.19041940569877625
05/20/2019 20:35:48 - INFO - __main__ -   epoch 1 step 28609 train loss:0.18231497704982758
05/20/2019 20:35:56 - INFO - __main__ -   epoch 1 step 28709 train loss:0.14158926904201508
05/20/2019 20:36:04 - INFO - __main__ -   epoch 1 step 28809 train loss:0.1533636450767517
05/20/2019 20:36:11 - INFO - __main__ -   epoch 1 step 28909 train loss:0.1139383465051651
05/20/2019 20:36:19 - INFO - __main__ -   epoch 1 step 29009 train loss:0.16536971926689148
05/20/2019 20:36:27 - INFO - __main__ -   epoch 1 step 29109 train loss:0.1823742687702179
05/20/2019 20:36:35 - INFO - __main__ -   epoch 1 step 29209 train loss:0.20683203637599945
05/20/2019 20:36:43 - INFO - __main__ -   epoch 1 step 29309 train loss:0.1091635599732399
05/20/2019 20:36:51 - INFO - __main__ -   epoch 1 step 29409 train loss:0.17783856391906738
05/20/2019 20:36:59 - INFO - __main__ -   epoch 1 step 29509 train loss:0.18368326127529144
05/20/2019 20:37:07 - INFO - __main__ -   epoch 1 step 29609 train loss:0.18930964171886444
05/20/2019 20:37:15 - INFO - __main__ -   epoch 1 step 29709 train loss:0.1699310541152954
05/20/2019 20:37:23 - INFO - __main__ -   epoch 1 step 29809 train loss:0.18947045505046844
05/20/2019 20:37:30 - INFO - __main__ -   epoch 1 step 29909 train loss:0.16776089370250702
05/20/2019 20:37:38 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:43:10 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_30000_predictions.json
05/20/2019 20:43:10 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_30000_nbest_predictions.json
05/20/2019 20:44:07 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:44:09 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:44:09 - INFO - __main__ -   epoch 1 step 30000 eval_loss: 3.3235511779785156 evaluate f1: 4.24747455255606 evaluate best f1:50.07399742513025
05/20/2019 20:44:10 - INFO - __main__ -   epoch 1 step 30009 train loss:0.07632239907979965
05/20/2019 20:44:17 - INFO - __main__ -   epoch 1 step 30109 train loss:0.15497158467769623
05/20/2019 20:44:25 - INFO - __main__ -   epoch 1 step 30209 train loss:0.150767982006073
05/20/2019 20:44:33 - INFO - __main__ -   epoch 1 step 30309 train loss:0.17985749244689941
05/20/2019 20:44:41 - INFO - __main__ -   epoch 1 step 30409 train loss:0.1439070850610733
05/20/2019 20:44:49 - INFO - __main__ -   epoch 1 step 30509 train loss:0.14436198770999908
05/20/2019 20:44:57 - INFO - __main__ -   epoch 1 step 30609 train loss:0.14402349293231964
05/20/2019 20:45:05 - INFO - __main__ -   epoch 1 step 30709 train loss:0.16185253858566284
05/20/2019 20:45:13 - INFO - __main__ -   epoch 1 step 30809 train loss:0.17682762444019318
05/20/2019 20:45:20 - INFO - __main__ -   epoch 1 step 30909 train loss:0.16288290917873383
05/20/2019 20:45:28 - INFO - __main__ -   epoch 1 step 31009 train loss:0.0900001972913742
05/20/2019 20:45:36 - INFO - __main__ -   epoch 1 step 31109 train loss:0.1862296462059021
05/20/2019 20:45:44 - INFO - __main__ -   epoch 1 step 31209 train loss:0.14979970455169678
05/20/2019 20:45:52 - INFO - __main__ -   epoch 1 step 31309 train loss:0.20903272926807404
05/20/2019 20:46:00 - INFO - __main__ -   epoch 1 step 31409 train loss:0.23411555588245392
05/20/2019 20:46:08 - INFO - __main__ -   epoch 1 step 31509 train loss:0.1892879605293274
05/20/2019 20:46:16 - INFO - __main__ -   epoch 1 step 31609 train loss:0.15585047006607056
05/20/2019 20:46:24 - INFO - __main__ -   epoch 1 step 31709 train loss:0.20980869233608246
05/20/2019 20:46:31 - INFO - __main__ -   epoch 1 step 31809 train loss:0.19663488864898682
05/20/2019 20:46:39 - INFO - __main__ -   epoch 1 step 31909 train loss:0.2124474048614502
05/20/2019 20:46:47 - INFO - __main__ -   epoch 1 step 32009 train loss:0.1656128466129303
05/20/2019 20:46:55 - INFO - __main__ -   epoch 1 step 32109 train loss:0.1427655816078186
05/20/2019 20:47:03 - INFO - __main__ -   epoch 1 step 32209 train loss:0.14494045078754425
05/20/2019 20:47:11 - INFO - __main__ -   epoch 1 step 32309 train loss:0.20731864869594574
05/20/2019 20:47:19 - INFO - __main__ -   epoch 1 step 32409 train loss:0.09737994521856308
05/20/2019 20:47:26 - INFO - __main__ -   epoch 1 step 32509 train loss:0.1585843712091446
05/20/2019 20:47:34 - INFO - __main__ -   epoch 1 step 32609 train loss:0.1669578105211258
05/20/2019 20:47:42 - INFO - __main__ -   epoch 1 step 32709 train loss:0.19737723469734192
05/20/2019 20:47:50 - INFO - __main__ -   epoch 1 step 32809 train loss:0.15712395310401917
05/20/2019 20:47:58 - INFO - __main__ -   epoch 1 step 32909 train loss:0.1611097902059555
05/20/2019 20:48:05 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 20:53:37 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_33000_predictions.json
05/20/2019 20:53:37 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_33000_nbest_predictions.json
05/20/2019 20:54:36 - INFO - __main__ -   start evaluation script.................
05/20/2019 20:54:38 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 20:54:38 - INFO - __main__ -   epoch 1 step 33000 eval_loss: 3.1823348999023438 evaluate f1: 4.614262910156456 evaluate best f1:50.08252016379701
05/20/2019 20:54:39 - INFO - __main__ -   epoch 1 step 33009 train loss:0.19835785031318665
05/20/2019 20:54:47 - INFO - __main__ -   epoch 1 step 33109 train loss:0.18901270627975464
05/20/2019 20:54:54 - INFO - __main__ -   epoch 1 step 33209 train loss:0.18027472496032715
05/20/2019 20:55:02 - INFO - __main__ -   epoch 1 step 33309 train loss:0.179817795753479
05/20/2019 20:55:10 - INFO - __main__ -   epoch 1 step 33409 train loss:0.19512192904949188
05/20/2019 20:55:18 - INFO - __main__ -   epoch 1 step 33509 train loss:0.18925094604492188
05/20/2019 20:55:26 - INFO - __main__ -   epoch 1 step 33609 train loss:0.19741323590278625
05/20/2019 20:55:33 - INFO - __main__ -   epoch 1 step 33709 train loss:0.17701666057109833
05/20/2019 20:55:41 - INFO - __main__ -   epoch 1 step 33809 train loss:0.12958107888698578
05/20/2019 20:55:49 - INFO - __main__ -   epoch 1 step 33909 train loss:0.21649198234081268
05/20/2019 20:55:57 - INFO - __main__ -   epoch 1 step 34009 train loss:0.20265443623065948
05/20/2019 20:56:05 - INFO - __main__ -   epoch 1 step 34109 train loss:0.20124927163124084
05/20/2019 20:56:13 - INFO - __main__ -   epoch 1 step 34209 train loss:0.08854653686285019
05/20/2019 20:56:21 - INFO - __main__ -   epoch 1 step 34309 train loss:0.09167788922786713
05/20/2019 20:56:29 - INFO - __main__ -   epoch 1 step 34409 train loss:0.14986324310302734
05/20/2019 20:56:36 - INFO - __main__ -   epoch 1 step 34509 train loss:0.1723768562078476
05/20/2019 20:56:44 - INFO - __main__ -   epoch 1 step 34609 train loss:0.16883791983127594
05/20/2019 20:56:52 - INFO - __main__ -   epoch 1 step 34709 train loss:0.16089963912963867
05/20/2019 20:57:00 - INFO - __main__ -   epoch 1 step 34809 train loss:0.18888364732265472
05/20/2019 20:57:08 - INFO - __main__ -   epoch 1 step 34909 train loss:0.20200839638710022
05/20/2019 20:57:16 - INFO - __main__ -   epoch 1 step 35009 train loss:0.16778837144374847
05/20/2019 20:57:24 - INFO - __main__ -   epoch 1 step 35109 train loss:0.1479351967573166
05/20/2019 20:57:32 - INFO - __main__ -   epoch 1 step 35209 train loss:0.1571265310049057
05/20/2019 20:57:40 - INFO - __main__ -   epoch 1 step 35309 train loss:0.1455138772726059
05/20/2019 20:57:47 - INFO - __main__ -   epoch 1 step 35409 train loss:0.14340388774871826
05/20/2019 20:57:55 - INFO - __main__ -   epoch 1 step 35509 train loss:0.1928798109292984
05/20/2019 20:58:03 - INFO - __main__ -   epoch 1 step 35609 train loss:0.17221060395240784
05/20/2019 20:58:11 - INFO - __main__ -   epoch 1 step 35709 train loss:0.17601127922534943
05/20/2019 20:58:19 - INFO - __main__ -   epoch 1 step 35809 train loss:0.136164590716362
05/20/2019 20:58:27 - INFO - __main__ -   epoch 1 step 35909 train loss:0.18636250495910645
05/20/2019 20:58:34 - INFO - __main__ -   Start predicton for evaluating..............
05/20/2019 21:04:08 - INFO - __main__ -   Writing predictions to: ../../output/log_eval/epoch_1_step_36000_predictions.json
05/20/2019 21:04:08 - INFO - __main__ -   Writing nbest to: ../../output/log_eval/epoch_1_step_36000_nbest_predictions.json
05/20/2019 21:05:03 - INFO - __main__ -   start evaluation script.................
05/20/2019 21:05:06 - INFO - examples.evaluate -   write evaluation result to ../../output/log_eval/eval_res/eval.jsonOK!
05/20/2019 21:05:06 - INFO - __main__ -   epoch 1 step 36000 eval_loss: 3.1399834156036377 evaluate f1: 4.826307818189781 evaluate best f1:50.0870322019147
05/20/2019 21:05:07 - INFO - __main__ -   epoch 1 step 36009 train loss:0.1825745850801468
05/20/2019 21:05:15 - INFO - __main__ -   epoch 1 step 36109 train loss:0.16905289888381958
05/20/2019 21:05:23 - INFO - __main__ -   epoch 1 step 36209 train loss:0.156260684132576
Terminated
